{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jinnic/ada-project1/blob/main/Project%201/Data_Cleaning_for_case_pridiction_using_PERM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHya1T8-BTU6",
        "outputId": "1fa37724-577f-4a42-a422-e731442c1c8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==1.4\n",
            "  Downloading scikit_learn-1.4.0-1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting numpy<2.0,>=1.19.5 (from scikit-learn==1.4)\n",
            "  Downloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.4) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.4) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn==1.4) (3.6.0)\n",
            "Downloading scikit_learn-1.4.0-1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.0/18.0 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "!pip install -U scikit-learn==1.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvlCWtDbr3wm"
      },
      "outputs": [],
      "source": [
        "github_data_url = 'https://raw.githubusercontent.com/jinnic/ada-project1/refs/heads/main/Project%201/raw_case_approve_2024_1.csv'\n",
        "\n",
        "# Read the CSV file directly from the GitHub URL\n",
        "df = pd.read_csv(github_data_url)\n",
        "\n",
        "df.head()\n",
        "\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHaSPwuxwAfZ"
      },
      "source": [
        "# Step 1:  Cleaning Dataset from PERM [2024 PERM data](https://www.dol.gov/sites/dolgov/files/ETA/oflc/pdfs/PERM_Disclosure_Data_New_Form_FY2024_Q4.xlsx)\n",
        "\n",
        "\n",
        "1.   **Filter out only yearly wage, fulltime, non-domestic help job**\n",
        "2.   **Create Avg wage feature with 'JOB_OPP_WAGE_FROM' &\t'JOB_OPP_WAGE_TO'**\n",
        "3. **Simplify NAICS code to first two digit**\n",
        "\n",
        "\n",
        "## **‚úÖ Filter out data**\n",
        "\n",
        "```\n",
        "'CASE_STATUS' == 'Certified' & 'Denied'\n",
        "'OCCUPATION_TYPE' == 'Professional occupation'\n",
        "'JOB_OPP_WAGE_PER' == 'Year'\n",
        "'OTHER_REQ_IS_FULLTIME_EMP' == 'Y'\n",
        "'OTHER_REQ_IS_LIVEIN_HOUSEHOLD' == 'N'\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVH35gJYcYet"
      },
      "outputs": [],
      "source": [
        "df_filter = df[\n",
        "    (df['CASE_STATUS'] == 'Certified') | (df['CASE_STATUS'] == 'Denied')&\n",
        "    (df['JOB_OPP_WAGE_PER'] == 'Year') &\n",
        "    (df['OTHER_REQ_IS_FULLTIME_EMP'] == 'Y') &\n",
        "    (df['OTHER_REQ_IS_LIVEIN_HOUSEHOLD'] == 'N')      # Exclude live-in workers\n",
        "].copy()\n",
        "\n",
        "# Drop NA for 'OTHER_REQ_IS_LIVEIN_HOUSEHOLD'\n",
        "df_filter.dropna(subset=['OTHER_REQ_IS_LIVEIN_HOUSEHOLD'], inplace=True)\n",
        "\n",
        "print(f\"Original row count: {len(df)}\")\n",
        "print(f\"Clean row count: {len(df_filter)}\")\n",
        "\n",
        "display(df_filter.head())\n",
        "\n",
        "print(df_filter.isna().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **‚úÖ Create Average wage colum**\n",
        "\n",
        "```\n",
        "'JOB_AVG_WAGE' is not empty # calculated with 'JOB_OPP_WAGE_FROM' &\n",
        "'JOB_OPP_WAGE_TO'\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GYF5AOP_hLAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_clean = df_filter.copy()\n",
        "\n",
        "# Convert relevant columns to numeric, stripping non-numeric characters\n",
        "for col in ['JOB_OPP_WAGE_FROM', 'JOB_OPP_WAGE_TO']:\n",
        "    df_clean[col] = df_clean[col].astype(str).str.replace('$', '', regex=False).str.replace(',', '', regex=False)\n",
        "    df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
        "\n",
        "# If 'JOB_OPP_WAGE_TO' is empty, replace it with 'JOB_OPP_WAGE_FROM'\n",
        "df_clean['JOB_OPP_WAGE_TO'].fillna(df_clean['JOB_OPP_WAGE_FROM'], inplace=True)\n",
        "\n",
        "# Create 'JOB_AVG_WAGE' by averaging the two columns\n",
        "df_clean['JOB_AVG_WAGE'] = (df_clean['JOB_OPP_WAGE_FROM'] + df_clean['JOB_OPP_WAGE_TO']) / 2\n",
        "\n",
        "# Drop null row of 'JOB_AVG_WAGE'\n",
        "print(f\"Original row count: {len(df_clean)}\")\n",
        "df_clean.dropna(subset=['JOB_AVG_WAGE','PRIMARY_WORKSITE_BLS_AREA'], inplace=True)\n",
        "print(f\"Clean row count: {len(df_clean)}\")\n",
        "\n",
        "print(df_clean.isna().sum())\n",
        "\n",
        "# Check calculation\n",
        "print(df_clean['JOB_AVG_WAGE'].dtype)\n",
        "display(df_clean[['JOB_OPP_WAGE_FROM', 'JOB_OPP_WAGE_TO', 'JOB_AVG_WAGE']].head(35))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "lqWMFsvCf5C8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##‚úÖ **Create join table for NAICS with Industry name**"
      ],
      "metadata": {
        "id": "9Qv6rmgnnD5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "naics_data = \"\"\"\n",
        "11\tAgriculture, Forestry, Fishing and Hunting\n",
        "21\tMining, Quarrying, and Oil and Gas Extraction\n",
        "22\tUtilities\n",
        "23\tConstruction\n",
        "31-33\tManufacturing\n",
        "42\tWholesale Trade\n",
        "44-45\tRetail Trade\n",
        "48-49\tTransportation and Warehousing\n",
        "51\tInformation\n",
        "52\tFinance and Insurance\n",
        "53\tReal Estate and Rental and Leasing\n",
        "54\tProfessional, Scientific, and Technical Services\n",
        "55\tManagement of Companies and Enterprises\n",
        "56\tAdministrative and Support and Waste Management and Remediation Services\n",
        "61\tEducational Services\n",
        "62\tHealth Care and Social Assistance\n",
        "71\tArts, Entertainment, and Recreation\n",
        "72\tAccommodation and Food Services\n",
        "81\tOther Services (except Public Administration)\n",
        "92\tPublic Administration\n",
        "\"\"\"\n",
        "\n",
        "naics_mapping = {}\n",
        "for line in naics_data.strip().split('\\n'):\n",
        "    parts = line.split('\\t')\n",
        "    code_str = parts[0]\n",
        "    sector_name = parts[1]\n",
        "\n",
        "    if '-' in code_str:\n",
        "        start, end = map(int, code_str.split('-'))\n",
        "        for i in range(start, end + 1):\n",
        "            naics_mapping[str(i)] = sector_name\n",
        "    else:\n",
        "        naics_mapping[code_str] = sector_name\n",
        "\n",
        "# Create a DataFrame from the mapping\n",
        "naics_df = pd.DataFrame(naics_mapping.items(), columns=['NAICS_CODE', 'NAICS_DESCRIPTION'])\n",
        "\n",
        "print(\"NAICS CODE Mapping DataFrame:\")\n",
        "display(naics_df.head(10))\n",
        "print(f\"Total unique NAICS CODES: {len(naics_df)}\")\n"
      ],
      "metadata": {
        "id": "26TXN3q2ldK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY005xycw-kv"
      },
      "source": [
        "## **‚úÖ Simplyfy NAICS - North American Industry Classification System**\n",
        "\n",
        "Keep first two digit for simpler category of industry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLQU_QdSufo-"
      },
      "outputs": [],
      "source": [
        "# Convert the EMP_NAICS column to a string\n",
        "# slice to keep first two digit\n",
        "df_clean['NAICS_CODE'] = df_clean['EMP_NAICS'].astype(str).str[0:2]\n",
        "\n",
        "# Check its unique values and count to make sure it's matching join table\n",
        "print(sorted(df_clean['NAICS_CODE'].unique()))\n",
        "print(f\"Total unique NAICS codes mapped: {len(df_clean['NAICS_CODE'].unique())}\")\n",
        "\n",
        "\n",
        "display(df_clean[['EMP_NAICS', 'NAICS_CODE']].head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge df_filter with naics_df to add the NAICS_DESCRIPTION\n",
        "df_clean = pd.merge(\n",
        "    df_clean,\n",
        "    naics_df,\n",
        "    left_on='NAICS_CODE',\n",
        "    right_on='NAICS_CODE', # Use NAICS_CODE from naics_df\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Display the first few rows to show the new 'NAICS_DESCRIPTION' column\n",
        "display(df_clean[['EMP_NAICS', 'NAICS_CODE', 'NAICS_DESCRIPTION']].head())\n",
        "\n",
        "# Check for any missing values in the new 'NAICS_DESCRIPTION' column\n",
        "print(f\"Missing values in NAICS_DESCRIPTION after merge: {df_clean['NAICS_DESCRIPTION'].isnull().sum()}\")"
      ],
      "metadata": {
        "id": "jkD8WUl8nsID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **‚úÖ Clean Job title**\n",
        "* Merge jr and junior, sr and senior\n",
        "* Keep similar software engineer job in same"
      ],
      "metadata": {
        "id": "DmJdEW5N34g7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkGY1Frn7A3i"
      },
      "outputs": [],
      "source": [
        "# Convert to lowercase and remove leading/trailing spaces\n",
        "df_clean['JOB_TITLE_CLEAN'] = df_clean['JOB_TITLE'].str.lower().str.strip()\n",
        "print(f\"Unique Titles before cleaning: {df_clean['JOB_TITLE'].nunique()}\")\n",
        "\n",
        "# remove common seniority prefixes/suffixes and numbers from title\n",
        "df_clean['JOB_TITLE_CLEAN'] = (\n",
        "    df_clean['JOB_TITLE_CLEAN']\n",
        "    .str.replace(r'\\b(sr|senior)\\b', 'sr.', regex=True)\n",
        "    .str.replace(r'\\b(jr|junior)\\b', 'jr.', regex=True)\n",
        "    .str.replace(r'\\d+', '', regex=True) # Remove numbers\n",
        "    .str.replace(r'[^\\w\\s]', '', regex=True) # Remove punctuation (e.g., commas, dashes)\n",
        "    .str.replace(r'\\s+', ' ', regex=True)     # Collapse multiple spaces into one\n",
        "    .str.strip()\n",
        ")\n",
        "\n",
        "# replace 'software development engineer' with 'software engineer'\n",
        "df_clean['JOB_TITLE_CLEAN'] = df_clean['JOB_TITLE_CLEAN'].str.replace(\n",
        "    'software development engineer',\n",
        "    'software engineer',\n",
        "    regex=True\n",
        ")\n",
        "\n",
        "# replace 'software developer' with 'software engineer'\n",
        "df_clean['JOB_TITLE_CLEAN'] = df_clean['JOB_TITLE_CLEAN'].str.replace(\n",
        "    'software developer',\n",
        "    'software engineer',\n",
        "    regex=True\n",
        ")\n",
        "\n",
        "# replace 'software programmer' with 'software engineer'\n",
        "df_clean['JOB_TITLE_CLEAN'] = df_clean['JOB_TITLE_CLEAN'].str.replace(\n",
        "    'software programmer',\n",
        "    'software engineer',\n",
        "    regex=True\n",
        ")\n",
        "\n",
        "print(f\"Unique Titles after cleaning: {df_clean['JOB_TITLE_CLEAN'].nunique()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚úÖ **Drop unneccesry columns**\n",
        "Columns that are filtered or used to create feature.\n",
        "\n",
        "* 'JOB_OPP_WAGE_TO' & 'JOB_OPP_WAGE_FROM'\n",
        "* 'JOB_OPP_WAGE_PER' == 'Year'\n",
        "* 'EMP_NAICS' <- full digit\n",
        "* 'OTHER_REQ_IS_FULLTIME_EMP' == 'Y'\n",
        "* 'OTHER_REQ_IS_LIVEIN_HOUSEHOLD' == 'N'\n",
        "\n"
      ],
      "metadata": {
        "id": "lY3hHpClkqxn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_drop = [\n",
        "    'JOB_OPP_WAGE_TO',\n",
        "    'JOB_OPP_WAGE_FROM',\n",
        "    'JOB_OPP_WAGE_PER',\n",
        "    'EMP_NAICS',\n",
        "    'OTHER_REQ_IS_FULLTIME_EMP',\n",
        "    'OTHER_REQ_IS_LIVEIN_HOUSEHOLD'\n",
        "]\n",
        "print(\"Columns dropped successfully.\")\n",
        "print(f\"Clean row count: {len(df_clean)}\")\n",
        "df_clean = df_clean.drop(columns=columns_to_drop)\n",
        "print(f\"Final row count: {len(df_clean)}\")\n",
        "\n",
        "display(df_clean.head())\n",
        "print(\"Missing values (NA count) per column:\")\n",
        "print(df_clean.isna().sum())"
      ],
      "metadata": {
        "id": "P4WP0yLto6tL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hXtEv9sLDR5"
      },
      "source": [
        "\n",
        "# Step 2: Divide into Training and Testing Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxD9lGCcL4rY"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df_final = df_clean.copy()\n",
        "\n",
        "# 1. Define the final X (Features) and Y (Target)\n",
        "Y = df_final['CASE_STATUS']\n",
        "X = df_final.drop(columns=['CASE_STATUS'])\n",
        "\n",
        "# 2. Split the data (80% Train, 20% Test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, Y,\n",
        "    test_size=0.2, # 20% for testing\n",
        "    random_state=42 # Ensures reproducible\n",
        ")\n",
        "\n",
        "print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}, y_test shape: {y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 3: Explore training set**"
      ],
      "metadata": {
        "id": "rgWQ0FithSre"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32Ge-uwUTLO6"
      },
      "outputs": [],
      "source": [
        "# Show the data type of the training features\n",
        "print(\"Training Features (X_train) Information:\")\n",
        "X_train.info()\n",
        "\n",
        "# Show the summary of the target variable (wage)\n",
        "print(\"\\nTarget (y_train) Summary:\")\n",
        "print(y_train.describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGaIBLHYQXPG"
      },
      "outputs": [],
      "source": [
        "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "print(\"Categorical features:\")\n",
        "print(categorical_features)\n",
        "\n",
        "print(\"\\nNumerical features:\")\n",
        "print(numerical_features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 4: Data cleaning**"
      ],
      "metadata": {
        "id": "JiGKa3x5JKTf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwialNyl0FuE"
      },
      "source": [
        "## ‚úÖ **Company Size & Tenure**\n",
        "Define the boundaries for company size categories and calculate the age of the company based on the year the dataset was published (Fiscal Year 2024)\n",
        "### **Thresholds common in business/economic:**\n",
        "\n",
        "* Small: < 50 employees\n",
        "* Mid: 50 to 1,000 employees\n",
        "* Large: > 1,000 employees\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "bins = [0, 50, 1000, np.inf]\n",
        "labels = ['Small', 'Mid', 'Large']\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3v5M2W-yznPL"
      },
      "outputs": [],
      "source": [
        "bins = [0, 50, 1000, np.inf]\n",
        "labels = ['Small', 'Mid', 'Large']\n",
        "\n",
        "X_train['COMPANY_SIZE_CAT'] = pd.cut(\n",
        "    X_train['EMP_NUM_PAYROLL'],\n",
        "    bins=bins,\n",
        "    labels=labels\n",
        ")\n",
        "\n",
        "X_test['COMPANY_SIZE_CAT'] = pd.cut(\n",
        "    X_train['EMP_NUM_PAYROLL'],\n",
        "    bins=bins,\n",
        "    labels=labels\n",
        ")\n",
        "\n",
        "print(\"\\nCompany Size Distribution:\")\n",
        "print(X_train['COMPANY_SIZE_CAT'].value_counts(dropna=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRFJ-Ajy3KnH"
      },
      "outputs": [],
      "source": [
        "##---------------create 'COMPANY_AGE' column\n",
        "# The dataset is from FY 2024\n",
        "\n",
        "REFERENCE_YEAR = 2024\n",
        "\n",
        "# Ensure EMP_YEAR_COMMENCED is treated as an integer (Year)\n",
        "X_train['EMP_YEAR_COMMENCED'] = pd.to_numeric(X_train['EMP_YEAR_COMMENCED'], errors='coerce')\n",
        "X_test['EMP_YEAR_COMMENCED'] = pd.to_numeric(X_test['EMP_YEAR_COMMENCED'], errors='coerce')\n",
        "\n",
        "# Calculate company age in years\n",
        "X_train['COMPANY_AGE'] = REFERENCE_YEAR - X_train['EMP_YEAR_COMMENCED']\n",
        "X_test['COMPANY_AGE'] = REFERENCE_YEAR - X_test['EMP_YEAR_COMMENCED']\n",
        "\n",
        "print(\"\\nCompany Age (Length of Establishment) Statistics:\")\n",
        "print(X_train['COMPANY_AGE'].describe())\n",
        "\n",
        "display(X_train['COMPANY_AGE'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GJh5z_g2J87"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# 1. Set the size of the plot\n",
        "plt.figure(figsize=(30, 6))\n",
        "\n",
        "# 2. Create the histogram\n",
        "plt.hist(X_train['COMPANY_AGE'], bins=50, edgecolor='black', color='skyblue')\n",
        "\n",
        "# 3. Add labels and a title for clarity\n",
        "plt.title('Distribution of Company Age (Years)')\n",
        "plt.xlabel('Company Age (Years)')\n",
        "plt.ylabel('Number of Applications')\n",
        "plt.grid(axis='y', alpha=0.5)\n",
        "\n",
        "# 4. Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "os8g-Wa53R1C"
      },
      "source": [
        "## ‚úÖ **Thresholds for company age based on distribution:**\n",
        "\n",
        "* Young: < 10 years old\n",
        "* Established: 10 to 25 years old\n",
        "* Mature: 25 to 45 years old\n",
        "* Legacy: > 45 years old\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "bins = [0, 10, 25, 45, np.inf]\n",
        "labels = ['Young', 'Established', 'Mature', 'Legacy']\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPEHGdWJ2uvx"
      },
      "outputs": [],
      "source": [
        "bins = [0, 10, 25, 45, np.inf]\n",
        "labels = ['Young', 'Established', 'Mature', 'Legacy']\n",
        "\n",
        "# Create the new categorical column\n",
        "X_train['COMPANY_AGE_CAT'] = pd.cut(\n",
        "    X_train['COMPANY_AGE'],\n",
        "    bins=bins,\n",
        "    labels=labels\n",
        ")\n",
        "\n",
        "X_test['COMPANY_AGE_CAT'] = pd.cut(\n",
        "    X_test['COMPANY_AGE'],\n",
        "    bins=bins,\n",
        "    labels=labels\n",
        ")\n",
        "\n",
        "# Display the new distribution\n",
        "print(\"\\nCompany Age Category Distribution (Custom Bins):\")\n",
        "print(X_train['COMPANY_AGE_CAT'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2_vPFToEzjQ"
      },
      "source": [
        "## ‚úÖ **Clean Job Titles on graph**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7adf421"
      },
      "outputs": [],
      "source": [
        "# Get the top 20 cleaned job titles\n",
        "top_n = 20\n",
        "top_job_titles = X_train['JOB_TITLE_CLEAN'].value_counts().nlargest(top_n)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(top_job_titles.index, top_job_titles.values, edgecolor='black', color='skyblue')\n",
        "plt.title(f'Top {top_n} Most Frequent Cleaned Job Titles After Merging')\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('Cleaned Job Title')\n",
        "plt.xticks(rotation=45, ha='right') # Rotate labels\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGw6Mj5vFA82"
      },
      "source": [
        "## ‚úÖ **Simplify Job title by grouping titles that appears > 50 times**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MezJYXVd6gPL"
      },
      "outputs": [],
      "source": [
        "#----- Simplify JOB_TITLE (Your core Job Feature) ---\n",
        "TITLE_THRESHOLD = 50  # Group titles that appear < 50 times\n",
        "title_counts = X_train['JOB_TITLE_CLEAN'].value_counts()\n",
        "rare_titles = title_counts[title_counts < TITLE_THRESHOLD].index\n",
        "X_train['JOB_TITLE_FINAL'] = X_train['JOB_TITLE_CLEAN'].replace(rare_titles, 'Other Job Title')\n",
        "X_test['JOB_TITLE_FINAL'] = X_test['JOB_TITLE_CLEAN'].replace(rare_titles, 'Other Job Title') # Apply to X_test\n",
        "print(f\"Final Job Title categories (X_train): {X_train['JOB_TITLE_FINAL'].nunique()}\")\n",
        "print(f\"Final Job Title categories (X_test): {X_test['JOB_TITLE_FINAL'].nunique()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nC6tS_P_FT3N"
      },
      "source": [
        "## ‚úÖ **Simplify PRIMARY_WORKSITE_BLS_AREA by grouping ares that is > 10 times**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFkIv3lyAAj7"
      },
      "outputs": [],
      "source": [
        "# --- Simplify PRIMARY_WORKSITE_BLS_AREA (Your core Location Feature) ---\n",
        "BLS_THRESHOLD = 100 # Group areas that appear < 50 times\n",
        "bls_counts = X_train['PRIMARY_WORKSITE_BLS_AREA'].value_counts()\n",
        "rare_bls = bls_counts[bls_counts < BLS_THRESHOLD].index\n",
        "X_train['BLS_AREA_FINAL'] = X_train['PRIMARY_WORKSITE_BLS_AREA'].replace(rare_bls, 'Other BLS Area')\n",
        "X_test['BLS_AREA_FINAL'] = X_test['PRIMARY_WORKSITE_BLS_AREA'].replace(rare_bls, 'Other BLS Area') # Apply to X_test\n",
        "print(f\"Final BLS Area categories (X_train): {X_train['BLS_AREA_FINAL'].nunique()}\")\n",
        "print(f\"Final BLS Area categories (X_test): {X_test['BLS_AREA_FINAL'].nunique()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "36b1bf82"
      },
      "outputs": [],
      "source": [
        "# Get the top 20 most frequent final BLS areas\n",
        "top_n = 20\n",
        "top_bls_areas = X_train['BLS_AREA_FINAL'].value_counts().nlargest(top_n)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "plt.bar(top_bls_areas.index, top_bls_areas.values, edgecolor='black', color='skyblue')\n",
        "plt.title(f'Top {top_n} Most Frequent Final BLS Areas')\n",
        "plt.xlabel('Final BLS Area')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Visualize the data in your training set"
      ],
      "metadata": {
        "id": "WBetA78LiPUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# One-hot encode the training target variable\n",
        "y_train_ohe = pd.get_dummies(y_train, prefix='CASE_STATUS', dtype=int)\n",
        "\n",
        "# One-hot encode the testing target variable\n",
        "y_test_ohe = pd.get_dummies(y_test, prefix='CASE_STATUS', dtype=int)\n",
        "\n",
        "print(\"Original y_train shape:\", y_train.shape)\n",
        "print(\"One-Hot Encoded y_train_ohe shape:\", y_train_ohe.shape)\n",
        "display(y_train_ohe.head())"
      ],
      "metadata": {
        "id": "uayBMzXWLOhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üìä 1. Target Variable Distribution (CASE_STATUS)**\n",
        "This code confirms the balance (or imbalance) of your target classes in y_train. This is crucial for classification."
      ],
      "metadata": {
        "id": "9cmneYBhMaXN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "# Plot the frequency of each CASE_STATUS category\n",
        "y_train.value_counts().plot(kind='bar', color=['skyblue', 'pink'])\n",
        "plt.title('Distribution of Target Y (CASE_STATUS) in Training Set')\n",
        "plt.xlabel('Case Status')\n",
        "plt.ylabel('# of Applications')\n",
        "plt.xticks(rotation=0) #horizontal label\n",
        "plt.grid(axis='y', alpha=0.45, color='purple')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WExIZr_mLTXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **üìä 2. Numerical Feature Distribution by Target Class (Box Plot)**"
      ],
      "metadata": {
        "id": "VnB7vdmoNEAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a temporary DataFrame combining X_train numerical features and y_train\n",
        "plot_df = X_train[['EMP_NUM_PAYROLL', 'COMPANY_AGE' ]].copy()\n",
        "plot_df['CASE_STATUS'] = y_train.values\n",
        "\n",
        "# Define the numerical features to plot\n",
        "numerical_features = ['EMP_NUM_PAYROLL', 'COMPANY_AGE']\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "for i, feature in enumerate(numerical_features):\n",
        "    plt.subplot(1, 2, i + 1)\n",
        "    # Boxplot of the numerical feature grouped by CASE_STATUS\n",
        "    plot_df.boxplot(column=feature, by='CASE_STATUS', ax=plt.gca(), showfliers=False)\n",
        "    plt.title(f'{feature} Distribution by Case Status (Outliers Hidden)')\n",
        "    plt.suptitle('') # Suppress the automatic suptitle created by pandas boxplot\n",
        "    plt.xlabel('Case Status')\n",
        "    plt.ylabel(feature)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4f-dQyoHLYgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Apply transformations to your data\n",
        "\n"
      ],
      "metadata": {
        "id": "9gfBk6botjp0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply the four transformations to your two core numerical features: EMP_NUM_PAYROLL and COMPANY_AGE."
      ],
      "metadata": {
        "id": "xRKlNDHWLhtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.plotting import scatter_matrix\n",
        "\n",
        "feature_1 = 'EMP_NUM_PAYROLL'\n",
        "feature_2 = 'COMPANY_AGE'\n",
        "\n",
        "## Transformations for EMP_NUM_PAYROLL\n",
        "\n",
        "# Squaring: EMP_NUM_PAYROLL^2\n",
        "X_train[f'{feature_1}_SQUARED'] = np.square(X_train[feature_1])\n",
        "# Cubing: EMP_NUM_PAYROLL^3\n",
        "X_train[f'{feature_1}_CUBED'] = np.power(X_train[feature_1], 3)\n",
        "# Logarithmic: log(EMP_NUM_PAYROLL + 1) (Adding 1 to handle zero payroll)\n",
        "X_train[f'{feature_1}_LOG'] = np.log(X_train[feature_1] + 1)\n",
        "# Exponential: e^(EMP_NUM_PAYROLL / 1000) (Scaled to prevent huge numbers)\n",
        "X_train[f'{feature_1}_EXP'] = np.exp(X_train[feature_1] / 1000)\n",
        "\n",
        "## Transformations for COMPANY_AGE\n",
        "\n",
        "# Squaring: COMPANY_AGE^2\n",
        "X_train[f'{feature_2}_SQUARED'] = np.square(X_train[feature_2])\n",
        "# Cubing: COMPANY_AGE^3\n",
        "X_train[f'{feature_2}_CUBED'] = np.power(X_train[feature_2], 3)\n",
        "# Logarithmic: log(COMPANY_AGE) (Adding 1e-9 to handle any age of 0 if it existed)\n",
        "X_train[f'{feature_2}_LOG'] = np.log(X_train[feature_2] + 1e-9)\n",
        "# Exponential: e^(COMPANY_AGE / 50) (Scaled by a moderate constant)\n",
        "X_train[f'{feature_2}_EXP'] = np.exp(X_train[feature_2] / 50)\n",
        "\n",
        "print(\"All four required transformations applied to both features in X_train.\")\n",
        "\n",
        "## Plot Histograms ##\n",
        "transformed_features_to_plot = [\n",
        "    f'{feature_1}_LOG', f'{feature_1}_SQUARED', f'{feature_2}_LOG', f'{feature_2}_SQUARED'\n",
        "]\n",
        "X_train[transformed_features_to_plot].hist(bins=30, figsize=(15, 8))\n",
        "plt.suptitle('Histograms of Transformed Features', y=1.05)\n",
        "plt.show()\n",
        "#\n",
        "\n",
        "## Plot Scatter Matrix ##\n",
        "# Select the log-transformed versions of the two features for the scatter matrix\n",
        "final_scatter_features = [\n",
        "    f'{feature_1}_LOG',\n",
        "    f'{feature_2}_LOG'\n",
        "]\n",
        "\n",
        "scatter_matrix(\n",
        "    X_train[final_scatter_features],\n",
        "    figsize=(12, 12),\n",
        "    diagonal='kde'\n",
        ")\n",
        "plt.suptitle('Scatter Matrix of Transformed EMP_NUM_PAYROLL and COMPANY_AGE', y=1.02)\n",
        "plt.show()\n",
        "#"
      ],
      "metadata": {
        "id": "PMixKyVVLeVN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Do the same for Test set"
      ],
      "metadata": {
        "id": "dMFsweNrOot1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_1 = 'EMP_NUM_PAYROLL'\n",
        "feature_2 = 'COMPANY_AGE'\n",
        "\n",
        "## Transformations for EMP_NUM_PAYROLL\n",
        "\n",
        "# Squaring: EMP_NUM_PAYROLL^2\n",
        "X_test[f'{feature_1}_SQUARED'] = np.square(X_test[feature_1])\n",
        "# Cubing: EMP_NUM_PAYROLL^3\n",
        "X_test[f'{feature_1}_CUBED'] = np.power(X_test[feature_1], 3)\n",
        "# Logarithmic: log(EMP_NUM_PAYROLL + 1)\n",
        "X_test[f'{feature_1}_LOG'] = np.log(X_test[feature_1] + 1)\n",
        "# Exponential: e^(EMP_NUM_PAYROLL / 1000)\n",
        "X_test[f'{feature_1}_EXP'] = np.exp(X_test[feature_1] / 1000)\n",
        "\n",
        "### Transformations for COMPANY_AGE\n",
        "\n",
        "# Squaring: COMPANY_AGE^2\n",
        "X_test[f'{feature_2}_SQUARED'] = np.square(X_test[feature_2])\n",
        "# Cubing: COMPANY_AGE^3\n",
        "X_test[f'{feature_2}_CUBED'] = np.power(X_test[feature_2], 3)\n",
        "# Logarithmic: log(COMPANY_AGE)\n",
        "X_test[f'{feature_2}_LOG'] = np.log(X_test[feature_2] + 1e-9) # Using 1e-9 for safety\n",
        "# Exponential: e^(COMPANY_AGE / 50)\n",
        "X_test[f'{feature_2}_EXP'] = np.exp(X_test[feature_2] / 50)\n",
        "\n",
        "print(\"All four required transformations successfully applied to X_test.\")\n",
        "print(f\"New X_test shape: {X_test.shape}\")"
      ],
      "metadata": {
        "id": "pM9-DZVIObMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One hot coding\n"
      ],
      "metadata": {
        "id": "ccvXMJaKvwBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode the target variable (CASE_STATUS)\n",
        "y_train_ohe = pd.get_dummies(y_train, prefix='CASE_STATUS', dtype=int)\n",
        "y_test_ohe = pd.get_dummies(y_test, prefix='CASE_STATUS', dtype=int)\n"
      ],
      "metadata": {
        "id": "ywEOvrw5LoNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Define the columns that will be one-hot encoded\n",
        "categorical_cols = [\n",
        "    'OCCUPATION_TYPE',\n",
        "    'PRIMARY_WORKSITE_TYPE',\n",
        "    'PRIMARY_WORKSITE_STATE',\n",
        "    'OTHER_REQ_JOB_FOREIGN_LANGUAGE',\n",
        "    'NAICS_DESCRIPTION',\n",
        "    'JOB_TITLE_FINAL',\n",
        "    'BLS_AREA_FINAL'\n",
        "]\n",
        "\n",
        "# Define the preprocessor using ColumnTransformer\n",
        "# 'passthrough' will keep all other numerical and proxy columns\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        # Apply OneHotEncoder to the selected categorical columns\n",
        "        ('onehot',\n",
        "         OneHotEncoder(sparse_output=False, handle_unknown='ignore'),\n",
        "         categorical_cols)\n",
        "    ],\n",
        "    # Keep all other columns (your numerical, log-transformed, and proxy features)\n",
        "    remainder='passthrough'\n",
        ")"
      ],
      "metadata": {
        "id": "S4CLiKHf_F9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the transformer on the training data\n",
        "preprocessor.fit(X_train)\n",
        "\n",
        "# Transform both training and test data\n",
        "X_train_processed = preprocessor.transform(X_train)\n",
        "X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "print(f\"Original X_train shape: {X_train.shape}\")\n",
        "print(f\"Processed X_train shape: {X_train_processed.shape}\")\n",
        "print(f\"Processed X_test shape: {X_test_processed.shape}\")"
      ],
      "metadata": {
        "id": "NxxJtLV-InbO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "history_visible": true,
      "authorship_tag": "ABX9TyOocrOYXMb7+xHkQoEhxn9y",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}